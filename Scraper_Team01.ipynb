{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Version 1\n",
    "  \n",
    "**This version can login to twitter automatically and navigate to the search bar, where it automatically chooses the \"latest\" tab for all historical tweet data. A snippet of javascript is used for automatic scrolling in order to load the next set of tweets. Also included is a way in order to tell  the page position and compare it each scroll, as to judge wether or not the end of all tweets is reached, much like in the advanced web scraping tutorial. In order to mitigate potential issues caused by a connection interruption / lag, a maximum number of 3 scroll attempts is allowed if current scroll position on the page = the last known scroll position.\n",
    "\n",
    "**With this version,a single keyword \"coronavaccin\" is being searched upon. Through use of the csv library it is then exported to a csv file.\n",
    "\n",
    "**Entities captured in this version so far are: \n",
    "    Tweet Text \n",
    "    User Name\n",
    "    Twitter Handle\n",
    "    Retweet Count\n",
    "    Like Count\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#VERSION 1\n",
    "\n",
    "\n",
    "import csv\n",
    "from getpass import getpass\n",
    "from time import sleep\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from msedge.selenium_tools import Edge, EdgeOptions \n",
    "\n",
    "#Function for getting multiple tweets\n",
    "\n",
    "def get_tweet_data(card):\n",
    "    username = card.find_element_by_xpath('.//span').text\n",
    "    try:\n",
    "        handle = card.find_element_by_xpath('.//span[contains(text(), \"@\")]').text\n",
    "    except NoSuchElementException:\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        postdate = card.find_element_by_xpath('.//time').get_attribute('datetime')\n",
    "    except NoSuchElementException:\n",
    "        return\n",
    "    \n",
    "    comment = card.find_element_by_xpath('.//div[2]/div[2]/div[1]').text\n",
    "    responding = card.find_element_by_xpath('.//div[2]/div[2]/div[2]').text\n",
    "    text = comment + responding\n",
    "    reply_cnt = card.find_element_by_xpath('.//div[@data-testid=\"reply\"]').text\n",
    "    retweet_cnt = card.find_element_by_xpath('.//div[@data-testid=\"retweet\"]').text\n",
    "    like_cnt = card.find_element_by_xpath('.//div[@data-testid=\"like\"]').text\n",
    "    \n",
    "    tweet = (username, handle, postdate, text, reply_cnt, retweet_cnt, like_cnt)\n",
    "    return tweet\n",
    "\n",
    "#Starting webdriver (using microsoft Edge)\n",
    "options = EdgeOptions()\n",
    "options.use_chromium = True\n",
    "driver = Edge(options=options)\n",
    "\n",
    "#Logging into Twitter\n",
    "driver.get('https://twitter.com/login') \n",
    "driver.maximize_window()\n",
    "\n",
    "username = driver.find_element_by_xpath('//input[@name=\"session[username_or_email]\"]')\n",
    "username.send_keys('Job_A98')\n",
    "\n",
    "my_password = getpass() \n",
    "\n",
    "password = driver.find_element_by_xpath('//input[@name = \"session[password]\"]')\n",
    "password.send_keys(my_password)\n",
    "password.send_keys(Keys.RETURN) \n",
    "sleep(1) \n",
    "\n",
    "#Finding search input\n",
    "\n",
    "search_input = driver.find_element_by_xpath('//input[@aria-label=\"Search query\"]')\n",
    "search_input.send_keys('#coronavaccin')\n",
    "search_input.send_keys(Keys.RETURN)\n",
    "\n",
    "#Going to \"Latest\" tab for historical data \n",
    "\n",
    "driver.find_element_by_link_text('Latest').click()  \n",
    "\n",
    "#Get tweets on page \n",
    "data = []\n",
    "tweet_ids = set() #Used in order to mitigate scraping duplicate tweets caused by possible increasing number of tweets per scroll \n",
    "last_position = driver.execute_script(\"return window.pageYOffset;\") #For tracking scroll position, breaking out of loop if end is reached\n",
    "scrolling = True\n",
    "\n",
    "while scrolling:\n",
    "    page_cards = driver.find_elements_by_xpath('//div[@data-testid=\"tweet\"]')\n",
    "    for card in page_cards[-15:]:\n",
    "        tweet = get_tweet_data(card)\n",
    "        if tweet:\n",
    "            tweet_id = ''.join(tweet)\n",
    "            if tweet_id not in tweet_ids:\n",
    "                tweet_ids.add(tweet_id)    \n",
    "                data.append(tweet)\n",
    "            \n",
    "            \n",
    "    scroll_attempt = 0 #used as sometimes due to lag scrolling will not register, so we allow for number of scroll attempts\n",
    "    while True:\n",
    "        driver.execute_script('window.scrollTo(0, document.body.scrollHeight);')\n",
    "        sleep(1) #giving program time to load before scraping\n",
    "        curr_position = driver.execute_script(\"return window.pageYOffset;\")\n",
    "        if last_position == curr_position: #breaks out of loop if current and last scroll positions are the same\n",
    "            scroll_attempt += 1 \n",
    "            if scroll_attempt >= 3:\n",
    "                scrolling = False\n",
    "                break\n",
    "            else: \n",
    "                sleep(2) \n",
    "        else:\n",
    "            last_position = curr_position\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('coronavaccin_tweets.csv', 'w', newline='', encoding='utf-8') as f:\n",
    "    header = ['UserName', 'Handle', 'Timestamp', 'Text', 'Reply count', 'Retweets', 'Like count']\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(header)\n",
    "    writer.writerows(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Version 2 \n",
    "\n",
    "**This version is a slightly altered version to version 1. As we attempt to eventually analyze sentiment towards a coronavirus vaccin during a certain time window around each covid pressconference (also for feasability, as the total number of tweets from the start of covid till now would mean a large number of data entries), we require searching around specific dates. In order to achieve this, this version starts the browser through selenium and navigates to a custom made URL (as in the adv web scraping tutorial) which contains specific time windows, enables capturing Dutch tweets only and yields tweets that include a multitude of ways in which people could misspell \"coronavaccin\" in order to get an as complete picture as possible. None of these things are taken into account in Version 1.  \n",
    "\n",
    "**The downside (?) to this approach is that we do need to re-run the scraper with a custom URL that captures the time windows around all previous press conferences. This would mean manually changing that URL for each time period and rerunning the scraper. \n",
    "\n",
    "**Entities captured in this version so far are: \n",
    "    Tweet Text \n",
    "    User Name\n",
    "    Twitter Handle\n",
    "    Retweet Count\n",
    "    Like Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#VERSION 2\n",
    "\n",
    "# Testing the scraper without automatic login. Here we made a custom search URL and inserted it into driver.get\n",
    "# The result should take into account specific dates posted (around the last persconferentie, Feb 22-24th 2021) and only contain Dutch tweets\n",
    "\n",
    "import csv\n",
    "from getpass import getpass\n",
    "from time import sleep\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from msedge.selenium_tools import Edge, EdgeOptions \n",
    "\n",
    "#Function for getting multiple tweets\n",
    "\n",
    "def get_tweet_data(card):\n",
    "    username = card.find_element_by_xpath('.//span').text\n",
    "    try:\n",
    "        handle = card.find_element_by_xpath('.//span[contains(text(), \"@\")]').text\n",
    "    except NoSuchElementException:\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        postdate = card.find_element_by_xpath('.//time').get_attribute('datetime')\n",
    "    except NoSuchElementException:\n",
    "        return\n",
    "    \n",
    "    comment = card.find_element_by_xpath('.//div[2]/div[2]/div[1]').text\n",
    "    responding = card.find_element_by_xpath('.//div[2]/div[2]/div[2]').text\n",
    "    text = comment + responding\n",
    "    reply_cnt = card.find_element_by_xpath('.//div[@data-testid=\"reply\"]').text\n",
    "    retweet_cnt = card.find_element_by_xpath('.//div[@data-testid=\"retweet\"]').text\n",
    "    like_cnt = card.find_element_by_xpath('.//div[@data-testid=\"like\"]').text\n",
    "    \n",
    "    tweet = (username, handle, postdate, text, reply_cnt, retweet_cnt, like_cnt)\n",
    "    return tweet\n",
    "\n",
    "#Starting webdriver with Microsoft Edge\n",
    "options = EdgeOptions()\n",
    "options.use_chromium = True\n",
    "driver = Edge(options=options)\n",
    "\n",
    "#Logging into Twitter\n",
    "driver.get('https://twitter.com/search?q=(coronavaccin%20OR%20corona_vaccin%20OR%20covidvaccin%20OR%20covid_vaccin%20OR%20corona_vaccine%20OR%20coronavaccine%20OR%20covidvaccine%20OR%20covid_vaccine)%20lang%3Anl%20until%3A2021-01-22%20since%3A2021-01-18&src=typed_query&f=live') \n",
    "driver.maximize_window() \n",
    "\n",
    "#Get tweets on page \n",
    "data = []\n",
    "tweet_ids = set() #Used in order to mitigate scraping duplicate tweets caused by possible increasing number of tweets per scroll \n",
    "last_position = driver.execute_script(\"return window.pageYOffset;\") #For tracking scroll position, breaking out of loop if end is reached\n",
    "scrolling = True\n",
    "\n",
    "while scrolling:\n",
    "    page_cards = driver.find_elements_by_xpath('//div[@data-testid=\"tweet\"]')\n",
    "    for card in page_cards[-15:]:\n",
    "        tweet = get_tweet_data(card)\n",
    "        if tweet:\n",
    "            tweet_id = ''.join(tweet)\n",
    "            if tweet_id not in tweet_ids:\n",
    "                tweet_ids.add(tweet_id)    \n",
    "                data.append(tweet)\n",
    "            \n",
    "            \n",
    "    scroll_attempt = 0 #used as sometimes due to lag scrolling will not register, so we allow for number of scroll attempts\n",
    "    while True:\n",
    "        driver.execute_script('window.scrollTo(0, document.body.scrollHeight);')\n",
    "        sleep(1) #giving program time to load before scraping\n",
    "        curr_position = driver.execute_script(\"return window.pageYOffset;\")\n",
    "        if last_position == curr_position: #breaks out of loop if current and last scroll positions are the same\n",
    "            scroll_attempt += 1 \n",
    "            if scroll_attempt >= 3:\n",
    "                scrolling = False\n",
    "                break\n",
    "            else: \n",
    "                sleep(2) \n",
    "        else:\n",
    "            last_position = curr_position\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('coronavaccin_tweets1.csv', 'w', newline='', encoding='utf-8') as f:\n",
    "    header = ['UserName', 'Handle', 'Timestamp', 'Text', 'Reply count', 'Retweets', 'Like count']\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(header)\n",
    "    writer.writerows(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Version 3 - CHROME\n",
    "# Scraping via chrome rather than edge.\n",
    "\n",
    "# The result should take into account specific dates posted (around the last persconferentie, Feb 22-24th 2021) and only contain Dutch tweets\n",
    "\n",
    "import csv\n",
    "from getpass import getpass\n",
    "from time import sleep\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium import webdriver\n",
    "\n",
    "#Function for getting multiple tweets\n",
    "\n",
    "def get_tweet_data(card):\n",
    "    username = card.find_element_by_xpath('.//span').text\n",
    "    try:\n",
    "        handle = card.find_element_by_xpath('.//span[contains(text(), \"@\")]').text\n",
    "    except NoSuchElementException:\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        postdate = card.find_element_by_xpath('.//time').get_attribute('datetime')\n",
    "    except NoSuchElementException:\n",
    "        return\n",
    "    \n",
    "    comment = card.find_element_by_xpath('.//div[2]/div[2]/div[1]').text\n",
    "    responding = card.find_element_by_xpath('.//div[2]/div[2]/div[2]').text\n",
    "    text = comment + responding\n",
    "    reply_cnt = card.find_element_by_xpath('.//div[@data-testid=\"reply\"]').text\n",
    "    retweet_cnt = card.find_element_by_xpath('.//div[@data-testid=\"retweet\"]').text\n",
    "    like_cnt = card.find_element_by_xpath('.//div[@data-testid=\"like\"]').text\n",
    "    \n",
    "    tweet = (username, handle, postdate, text, reply_cnt, retweet_cnt, like_cnt)\n",
    "    return tweet\n",
    "\n",
    "#Starting webdriver with Google Chrome\n",
    "browser = webdriver.Chrome()\n",
    "\n",
    "#Logging into Twitter\n",
    "browser.get('https://twitter.com/search?q=(coronavaccin%20OR%20corona_vaccin%20OR%20covidvaccin%20OR%20covid_vaccin%20OR%20corona_vaccine%20OR%20coronavaccine%20OR%20covidvaccine%20OR%20covid_vaccine)%20lang%3Anl%20until%3A2021-01-22%20since%3A2021-01-18&src=typed_query&f=live') \n",
    "browser.maximize_window() \n",
    "\n",
    "#Get tweets on page \n",
    "data = []\n",
    "tweet_ids = set() #Used in order to mitigate scraping duplicate tweets caused by possible increasing number of tweets per scroll \n",
    "last_position = browser.execute_script(\"return window.pageYOffset;\") #For tracking scroll position, breaking out of loop if end is reached\n",
    "scrolling = True\n",
    "\n",
    "while scrolling:\n",
    "    page_cards = browser.find_elements_by_xpath('//div[@data-testid=\"tweet\"]')\n",
    "    for card in page_cards[-15:]:\n",
    "        tweet = get_tweet_data(card)\n",
    "        if tweet:\n",
    "            tweet_id = ''.join(tweet)\n",
    "            if tweet_id not in tweet_ids:\n",
    "                tweet_ids.add(tweet_id)    \n",
    "                data.append(tweet)\n",
    "            \n",
    "            \n",
    "    scroll_attempt = 0 #used as sometimes due to lag scrolling will not register, so we allow for number of scroll attempts\n",
    "    while True:\n",
    "        browser.execute_script('window.scrollTo(0, document.body.scrollHeight);')\n",
    "        sleep(1) #giving program time to load before scraping\n",
    "        curr_position = browser.execute_script(\"return window.pageYOffset;\")\n",
    "        if last_position == curr_position: #breaks out of loop if current and last scroll positions are the same\n",
    "            scroll_attempt += 1 \n",
    "            if scroll_attempt >= 3:\n",
    "                scrolling = False\n",
    "                break\n",
    "            else: \n",
    "                sleep(2) \n",
    "        else:\n",
    "            last_position = curr_position\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('coronavaccin_tweets1.csv', 'w', newline='', encoding='utf-8') as f:\n",
    "    header = ['UserName', 'Handle', 'Timestamp', 'Text', 'Reply count', 'Retweets', 'Like count']\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(header)\n",
    "    writer.writerows(data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
